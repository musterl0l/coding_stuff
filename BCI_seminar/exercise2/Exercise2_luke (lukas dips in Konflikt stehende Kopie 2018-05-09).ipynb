{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 2: Neural Networks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def logistic(h,a=1):\n",
    "    return 1/(1+np.exp(-a*h))\n",
    "def dlogistic(h,a=1):\n",
    "    return logistic(h,a)-np.power(logistic(h,a),2)\n",
    "\n",
    "signtrafunc=lambda x: (np.sign(x)+1)/2\n",
    "\n",
    "class neuron:    \n",
    "    \n",
    "    def __init__(self,w,b=0,trafunc=logistic,dtrafunc=dlogistic):\n",
    "        self.w=np.array(w)\n",
    "        self.b=np.array(b)\n",
    "        self.trafunc=trafunc\n",
    "        self.dtrafunc=dtrafunc\n",
    "        \n",
    "    def out(self, x):\n",
    "        return self.trafunc(np.dot(self.w,x)-self.b)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Gradient Descent on a neuron (3 points - programming)\n",
    "Rewrite the neuron class given and include the Gradient Descent training algorithm as a method. To this extent, concentrate on each neuron individually by giving it the delta of the neurons connected to it's ouptut and the corresponding weights as parameters.\n",
    "\n",
    "Also, add the derivative of the transfer function as a method and safe the last input $x$, last output $y$, last activation $h$ and last delta $\\delta$ values as object attributes, whenever they are recalculated.\n",
    "The values of $h$,$x$,$y$ and $o$ are all to be taken as those of the current training data (= their last calculation).\n",
    "\n",
    "The forumla you need is: $\\delta_i = f'(h_i) \\sum_k w_k \\delta_k$, where i is the index of the current neuron and k is the index of the following neurons (connected to it's output). This formulation is needed to be able to use it for the Backpropagation later on. For a single neuron as for the last layer neurons, the delta of the following neuron is replaced by the error graident of the output $\\delta_k=\\left(y-o\\right)$ and the corresponding weight is $w_k=1$ as every output neuron in the model has only one unscaled output.\n",
    "The train function updates the weights by the gradient  descent weight update rule $w(t+1)=w(t)-\\eta\\delta_i(t)\\cdot x(t)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class neuron:    \n",
    "    \n",
    "    def __init__(self,w,b,trafunc=logistic,dtrafunc= dlogistic ):\n",
    "        self.w=np.array(w)\n",
    "        self.b=np.array(b)\n",
    "        self.trafunc=trafunc\n",
    "        self.dtrafunc=dtrafunc\n",
    "    def out(self, x):\n",
    "        return self.trafunc(np.dot(self.w,x)-self.b)   \n",
    "    \n",
    "    def delta(self,deltanext,weightsnext):\n",
    "        \n",
    "        return self.lastdelta\n",
    "    \n",
    "    def train(self,deltanext,weightsnext,learnrate=0.1):\n",
    "\n",
    "        return self.lastdelta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: logical functions (2 points - programming)\n",
    "Train a single neuron on the logical functions below for a two-dimensional input $x$. Use instances of the neuron class above to build the equivalents to logical \"or\",\"and\" and \"xor\"-functions and test them for 2-dimensional input vectors *x* resembling all possibilities of combinations ([0,0] [1,0], [0,1], [1,1]). Do 10.000 iterations and plot the evolution of the error (the error over the iteration number). You don't need to implement a stopping criterion.\n",
    "\n",
    "Set the learning rate to $\\eta=1$ and initialize the weight $w$ and the bias $b$ randomly with normal distribution (np.random.randn). Use the logistic function.\n",
    "\n",
    "In the next cell you find an exemplary random number generator and the corresponding functions you can use for sample creation in every single iteration. In every iteration use the random input $x$, the neuron output *$y=$neuron.out($x$)* and the training data $o=yourlogicalfunction(x)$\n",
    "\n",
    "If you haven't succeeded with task 1, you can import Exercise2helperPy36 or Exercise2helperPy27 depending on your python version. The syntax of the conained neuron class is: n1=Exercise2helper.neuron(np.random.randn(2),np.random.randn(1)) for initialization and train(deltanext,weightsnext,learnrate=0.1) for training. For a single neuron as for the last layer neurons, the delta of the following neuron (deltanext) is replaced by the error graident of the output $\\delta_k=\\left(y-o\\right)$ and the corresponding weight is $w_k=1$ as every output neuron in the model has only one unscaled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Exercise2helper36 as helper\n",
    "\n",
    "#for one single point\n",
    "x=np.random.randint(2,size=2)\n",
    "o=x[0] & x[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input vektor\n",
    "x_in = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_or = np.array([0,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) AND function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13987154])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize\n",
    "n1 = helper.neuron(np.random.randn(2),np.random.randn(1))\n",
    "\n",
    "y = n1.out(x)\n",
    "o=x[0] & x[1]\n",
    "deltanext = y - o\n",
    "weightsnext = 1\n",
    "n1.train(deltanext, weightsnext, learnrate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it = 10000\n",
    "error = np.zeros(it)\n",
    "\n",
    "for i in range(it):\n",
    "    error[i] = deltanext\n",
    "    x=np.random.rand10int(2,size=2)\n",
    "    o=x[0] & x[1]\n",
    "    y = n1.out(x)\n",
    "    \n",
    "    deltanext = y - o\n",
    "    weightsnext = 1\n",
    "    n1.train(deltanext, weightsnext, learnrate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03517624,  0.03509529, -0.03882485, ...,  0.02261149,\n",
       "        0.0226434 , -0.02912389])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) OR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 ^ 1 #xor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52497918747894001"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: MLP layer (2 points - programming)\n",
    "Use the class \"neuron\" to construct a neural layer class \"MLPlayer\" for a Multi-Layer Perceptron (MLP).\n",
    "It should contain a list \"MLPlayer.nodes\" which is a list of the single neurons. \n",
    "Also, there should be a method \"MLPlayer.out(x)\" that returns the outputs of the single neurons as a list for the different neurons' current weights and bias of the input vector \"x\". Initialize the weights and the biases of the single neurons randomly with normal distribution by default (np.random.randn()).\n",
    "\n",
    "Include a method \"MLPlayer.train(deltanext,W,learnrate)\" which iterates the training over the nodes by calling their \"train()\" method with the deltas of the nextlayers and the corresponding weights. W is matrix consisiting of the weights of of all neurons in the next layer. It should return the deltas and weight matrix W of the current layer.\n",
    "\n",
    "The number of outputs is equivalent to the number of nodes in the layer and the number of inputs corresponds with the number of weights per neuron. The number of neurons and the number of weights per neuron should be passed to the initialization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPlayer:\n",
    "    \n",
    "    def __init__(self,NodeNo,WeightNo,weightinit=np.random.randn,biasinit=np.random.randn,trafunc=logistic,dtrafunc=dlogistic):\n",
    "        \n",
    "    def out(self,x):\n",
    "        \n",
    "    def train(self,deltanext,W,learnrate=0.1):        \n",
    "\n",
    "        return deltas, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Backpropagation in MLP (7 points - programming)\n",
    "\n",
    "Construct a class \"MLP\" with a list of layers of type \"MLPlayer\" called \"MLP.layers\" Also, there should be a method \"MLP.out(x)\" that returns the outputs of the whole network of the input vector \"x\".\n",
    "\n",
    "Make sure, that the size of the weight vector is bound to the number of inputs for the first layer and that the number of inputs for the following layers has to correspond with the number of neurons in the layer before. The number of outputs equals the number of neurons in the last layer.\n",
    "\n",
    "* The init function gets the number of inputs *x* and the number of nodes for each layer as a list. The number of neurons per layer and the number of inputs should be passed to the initialization method.\n",
    "\n",
    "* Include the backpropagation training algorithm as a method \"MLP.train()\" into the class. The passed arguments should consist of the number of iteations (no stopping criteria in this case), the training input and the training output - both as function pointers - as well as the learning rate. It should iterate over the layers, which themselves iterate over their neurons. Deltas and W will allways be the input to the previous layer.\n",
    "\n",
    "* The function x_train has to produce a random array of inputs of size [NoInputs,] and o_train has to produce the corresponding target function output. They should work without an argument passed and the target training output $o$ should be calculated using only that vector $x$.\n",
    "\n",
    "* The output of the method *train()* should consist of the sum-of-squares error within each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 Train an MLP on the XOR (1 point - programming)\n",
    "Train a Multi-Layer-Perceptron on the logical \"xor\"-function. Do 10.000 iterations and plot the evolution of the error. You don't need to implement a stopping criterion. Use the logistic function.\n",
    "\n",
    "Set the learning rate to $\\eta=1$.\n",
    "\n",
    "Investigate the following steps:\n",
    "\n",
    "a) The network should consist of two layers, where the first has the two input neurons and the second only one output neuron. Does it always converge?\n",
    "\n",
    "b) The network should consist of two layers, where the first has the three  input neurons and the second only one output neuron. Does it now always converge?\n",
    "\n",
    "What can we learn from this?\n",
    "\n",
    "If you haven't succeeded with task 5, you can import Exercise2helperPy36 or Exercise2helperPy27 (depending on your python version 3.6 or 2.7). The syntax of the contained MLP class is:\n",
    "*NeuralNetwork=MLP(NoInputs,ListNoNeuronsPerLayer)*\n",
    "for initialization and \n",
    "*errors=NeuralNetwork.train(NoIterations,x, o ,learnrate)* for training. $x$ and $o$ can either be functions as defined in task 5 or arrays of samples. If they are functions, $x$ has to produce a random array of inputs of size [NoInputs,] and $o(x)$ has to produce the corresponding target function output. If they are samples they have to have the shape *x.shape=[NoIterations,NoInputs]* and *o.shape=[NoIterations,NoOutputs]*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
