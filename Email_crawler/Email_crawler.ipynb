{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import requests.exceptions\n",
    "from urllib.parse import urlsplit\n",
    "from collections import deque\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing http://www.themoscowtimes.com/contact_us/index.php\n",
      "Processing https://lightninghackday.fulmo.org/\n",
      "Processing http://pro-retail.de/en/about-us.html\n",
      "ready\n"
     ]
    }
   ],
   "source": [
    "# a queue of urls to be crawled\n",
    "new_urls = deque(['http://www.themoscowtimes.com/contact_us/index.php','https://lightninghackday.fulmo.org/','http://pro-retail.de/en/about-us.html'])\n",
    "\n",
    "# a set of urls that we have already crawled\n",
    "processed_urls = set()\n",
    "\n",
    "# a set of crawled emails\n",
    "emails = set()\n",
    "\n",
    "# process urls one by one until we exhaust the queue\n",
    "while len(new_urls):\n",
    "\n",
    "    # move next url from the queue to the set of processed urls\n",
    "    url = new_urls.popleft()\n",
    "    processed_urls.add(url)\n",
    "\n",
    "    # extract base url to resolve relative links\n",
    "    parts = urlsplit(url)\n",
    "    base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "    path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
    "\n",
    "    # get url's content\n",
    "    print(\"Processing %s\" % url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
    "        # ignore pages with errors\n",
    "        continue\n",
    "\n",
    "    # extract all email addresses and add them into the resulting set\n",
    "    new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", response.text, re.I))\n",
    "    emails.update(new_emails)\n",
    "'''\n",
    "    # create a beutiful soup for the html document\n",
    "    soup = BeautifulSoup(response.text)\n",
    "\n",
    "    # find and process all the anchors in the document\n",
    "    for anchor in soup.find_all(\"a\"):\n",
    "        # extract link url from the anchor\n",
    "        link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else ''\n",
    "        # resolve relative links\n",
    "        if link.startswith('/'):\n",
    "            link = base_url + link\n",
    "        elif not link.startswith('http'):\n",
    "            link = path + link\n",
    "        # add the new url to the queue if it was not enqueued nor processed yet\n",
    "        if not link in new_urls and not link in processed_urls:\n",
    "            new_urls.append(link)\n",
    "            \n",
    "'''\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = open('result_list.csv',\"w\")\n",
    "for e in emails:\n",
    "    row = e +','+'\\n'\n",
    "    #print(row)\n",
    "    csv.write(row)\n",
    "csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
